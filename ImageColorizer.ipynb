{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DeviceId.CPU: 99>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NOTE:  This must be the first call in order to work properly!\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "\n",
    "#choices:  CPU, GPU0...GPU7\n",
    "device.set(device=DeviceId.CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jayaprakash\\anaconda3\\envs\\Reinforcement_Learning\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\Jayaprakash\\anaconda3\\envs\\Reinforcement_Learning\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\Jayaprakash\\anaconda3\\envs\\Reinforcement_Learning\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "from deoldify.visualize import *\n",
    "torch.backends.cudnn.benchmark=True\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*?Your .*? set is empty.*?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:  Set artistic to False if you're having trouble getting a good render.  Chances are it will work with the Stable model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jayaprakash\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jayaprakash\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "colorizer = get_image_colorizer(artistic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "### source_url\n",
    "Type in a url to a direct link of an image.  Usually that means they'll end in .png, .jpg, etc.  NOTE: If you want to use your own image, you can set source_url to None and just upload the image to /test_images/ in Jupyter.  Just make sure that the source_path parameter matches the file you uploaded.\n",
    "\n",
    "### source_path\n",
    "Name this whatever sensible image path (plus extension of jpg/png/ext) you want!  Sensible means the path exists and the file exists if source_url=None.\n",
    "\n",
    "### render_factor\n",
    "The default value of 35 has been carefully chosen and should work -ok- for most scenarios (but probably won't be the -best-). This determines resolution at which the color portion of the image is rendered. Lower resolution will render faster, and colors also tend to look more vibrant. Older and lower quality images in particular will generally benefit by lowering the render factor. Higher render factors are often better for higher quality images, but the colors may get slightly washed out. \n",
    "\n",
    "### result_path\n",
    "Ditto- don't change.\n",
    "\n",
    "### How to Download a Copy\n",
    "Simply shift+right click on the displayed image and click \"Save Image As...\"!\n",
    "\n",
    "\n",
    "## Pro Tips\n",
    "1. You can evaluate how well the image is rendered at each render_factor by using the code at the bottom (that cell under \"See how well render_factor values perform on a frame here\"). \n",
    "2. Keep in mind again that you can go up top and set artistic to False for the colorizer to use the 'Stable' model instead.  This will often tend to do better on portraits, and natural landscapes.  \n",
    "\n",
    "\n",
    "## Troubleshooting\n",
    "If you get a 'CUDA out of memory' error, you probably have the render_factor too high.  The max is 45 on 11GB video cards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colorize!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Jayaprakash\\\\SIP Project\\\\Color-Restoration-Using-DeepLearning\\\\DeOldify\\\\test_images\\\\Thatha_bw.jpeg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Jayaprakash\\SIP Project\\Color-Restoration-Using-DeepLearning\\DeOldify\\ImageColorizer.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jayaprakash/SIP%20Project/Color-Restoration-Using-DeepLearning/DeOldify/ImageColorizer.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     result_path \u001b[39m=\u001b[39m colorizer\u001b[39m.\u001b[39mplot_transformed_image_from_url(url\u001b[39m=\u001b[39msource_url, path\u001b[39m=\u001b[39msource_path, render_factor\u001b[39m=\u001b[39mrender_factor, compare\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jayaprakash/SIP%20Project/Color-Restoration-Using-DeepLearning/DeOldify/ImageColorizer.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Jayaprakash/SIP%20Project/Color-Restoration-Using-DeepLearning/DeOldify/ImageColorizer.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     result_path \u001b[39m=\u001b[39m colorizer\u001b[39m.\u001b[39;49mplot_transformed_image(path\u001b[39m=\u001b[39;49msource_path, render_factor\u001b[39m=\u001b[39;49mrender_factor, compare\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jayaprakash/SIP%20Project/Color-Restoration-Using-DeepLearning/DeOldify/ImageColorizer.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m show_image_in_notebook(result_path)\n",
      "File \u001b[1;32mc:\\Users\\Jayaprakash\\SIP Project\\Color-Restoration-Using-DeepLearning\\DeOldify\\deoldify\\visualize.py:105\u001b[0m, in \u001b[0;36mModelImageVisualizer.plot_transformed_image\u001b[1;34m(self, path, results_dir, figsize, render_factor, display_render_factor, compare, post_process, watermarked)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mif\u001b[39;00m results_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     results_dir \u001b[39m=\u001b[39m Path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults_dir)\n\u001b[1;32m--> 105\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_transformed_image(\n\u001b[0;32m    106\u001b[0m     path, render_factor, post_process\u001b[39m=\u001b[39;49mpost_process,watermarked\u001b[39m=\u001b[39;49mwatermarked\n\u001b[0;32m    107\u001b[0m )\n\u001b[0;32m    108\u001b[0m orig \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_open_pil_image(path)\n\u001b[0;32m    109\u001b[0m \u001b[39mif\u001b[39;00m compare:\n",
      "File \u001b[1;32mc:\\Users\\Jayaprakash\\SIP Project\\Color-Restoration-Using-DeepLearning\\DeOldify\\deoldify\\visualize.py:173\u001b[0m, in \u001b[0;36mModelImageVisualizer.get_transformed_image\u001b[1;34m(self, path, render_factor, post_process, watermarked)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_transformed_image\u001b[39m(\n\u001b[0;32m    169\u001b[0m     \u001b[39mself\u001b[39m, path: Path, render_factor: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, post_process: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    170\u001b[0m     watermarked: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    171\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Image:\n\u001b[0;32m    172\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clean_mem()\n\u001b[1;32m--> 173\u001b[0m     orig_image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open_pil_image(path)\n\u001b[0;32m    174\u001b[0m     filtered_image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilter\u001b[39m.\u001b[39mfilter(\n\u001b[0;32m    175\u001b[0m         orig_image, orig_image, render_factor\u001b[39m=\u001b[39mrender_factor,post_process\u001b[39m=\u001b[39mpost_process\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m     \u001b[39mif\u001b[39;00m watermarked:\n",
      "File \u001b[1;32mc:\\Users\\Jayaprakash\\SIP Project\\Color-Restoration-Using-DeepLearning\\DeOldify\\deoldify\\visualize.py:58\u001b[0m, in \u001b[0;36mModelImageVisualizer._open_pil_image\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_pil_image\u001b[39m(\u001b[39mself\u001b[39m, path: Path) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Image:\n\u001b[1;32m---> 58\u001b[0m     \u001b[39mreturn\u001b[39;00m PIL\u001b[39m.\u001b[39;49mImage\u001b[39m.\u001b[39;49mopen(path)\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jayaprakash\\anaconda3\\envs\\Reinforcement_Learning\\lib\\site-packages\\PIL\\Image.py:3068\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3065\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[0;32m   3067\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[1;32m-> 3068\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   3069\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   3071\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Jayaprakash\\\\SIP Project\\\\Color-Restoration-Using-DeepLearning\\\\DeOldify\\\\test_images\\\\Thatha_bw.jpeg'"
     ]
    }
   ],
   "source": [
    "#NOTE:  Max is 45 with 11GB video cards. 35 is a good default\n",
    "render_factor=10\n",
    "#NOTE:  Make source_url None to just read from file at ./video/source/[file_name] directly without modification\n",
    "source_url=None\n",
    "source_path = 'DeOldify/test_images/Thatha_bw.jpeg'\n",
    "result_path = None\n",
    "\n",
    "if source_url is not None:\n",
    "    result_path = colorizer.plot_transformed_image_from_url(url=source_url, path=source_path, render_factor=render_factor, compare=True)\n",
    "else:\n",
    "    result_path = colorizer.plot_transformed_image(path=source_path, render_factor=render_factor, compare=True)\n",
    "\n",
    "show_image_in_notebook(result_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See how well render_factor values perform on the image here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(10,46):\n",
    "    #colorizer.plot_transformed_image(source_path, render_factor=i, display_render_factor=True, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Reinforcement_Learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "67px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "vscode": {
   "interpreter": {
    "hash": "afbd0340a3c788cc9b6f5c162b3e9fcb8c6e6f01f591b79facbdd7f88ebe99ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
